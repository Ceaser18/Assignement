{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg8NxZEoFUVX",
        "outputId": "7a2479c6-9be3-4b39-d687-d7688cf7f469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=23bf6eebcbef53abf9a108f7ecef699598cf173bdbf9d054b4d36dface7d37eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install libraries needed for RSS parsing, NLP, and text processing\n",
        "!pip install feedparser nltk pandas numpy scikit-learn beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries for RSS parsing, NLP, and text processing\n",
        "import feedparser\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Download NLTK data for sentence tokenization and stopwords\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QllJBUAaHwNz",
        "outputId": "f12c76ac-ceb4-4d14-fabc-95e950c80ed7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RSS feeds for each category with updated finance feeds\n",
        "RSS_FEEDS = {\n",
        "    'general': ['http://feeds.bbci.co.uk/news/world/rss.xml', 'https://rss.nytimes.com/services/xml/rss/nyt/World.xml', 'http://feeds.reuters.com/reuters/topNews'],\n",
        "    'technology': ['https://techcrunch.com/feed/', 'https://www.wired.com/feed/rss', 'https://www.technologyreview.com/feed/'],\n",
        "    'finance': [\n",
        "        'http://feeds.reuters.com/reuters/businessNews',  # Reuters Finance\n",
        "        'https://www.ft.com/?format=rss',  # Financial Times\n",
        "        'https://finance.yahoo.com/news/rss'  # Yahoo Finance\n",
        "    ],\n",
        "    'sports': ['https://www.espn.com/espn/rss/news', 'http://feeds.bbci.co.uk/sport/rss.xml', 'https://www.skysports.com/rss'],\n",
        "    'entertainment': ['https://variety.com/feed/', 'https://www.hollywoodreporter.com/feed/', 'https://www.billboard.com/feed/'],\n",
        "    'science': ['https://www.nasa.gov/rss/dyn/breaking_news.rss', 'https://www.sciencedaily.com/rss/all.xml', 'https://arstechnica.com/feed/']\n",
        "}\n",
        "\n",
        "# Define user personas with their interests and categories\n",
        "USER_PERSONAS = {\n",
        "    'Alex Parker': {'interests': ['AI', 'cybersecurity', 'blockchain', 'startups', 'programming'], 'categories': ['technology']},\n",
        "    'Priya Sharma': {'interests': ['global markets', 'startups', 'fintech', 'cryptocurrency', 'economics'], 'categories': ['finance']},\n",
        "    'Marco Rossi': {'interests': ['football', 'F1', 'NBA', 'olympic sports', 'esports'], 'categories': ['sports']},\n",
        "    'Lisa Thompson': {'interests': ['movies', 'celebrity news', 'TV shows', 'music', 'books'], 'categories': ['entertainment']},\n",
        "    'David Martinez': {'interests': ['space exploration', 'AI', 'biotech', 'physics', 'renewable energy'], 'categories': ['science', 'technology']}\n",
        "}"
      ],
      "metadata": {
        "id": "n9N9NIx3H_mH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsletterGenerator:\n",
        "    def __init__(self):\n",
        "        self.articles = []\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    def fetch_articles(self):\n",
        "        \"\"\"Fetch articles from RSS feeds\"\"\"\n",
        "        for category, feeds in RSS_FEEDS.items():\n",
        "            for feed_url in feeds:\n",
        "                try:\n",
        "                    feed = feedparser.parse(feed_url)\n",
        "                    for entry in feed.entries[:10]:  # Limit to 10 articles per feed\n",
        "                        article = {\n",
        "                            'title': entry.get('title', ''),\n",
        "                            'link': entry.get('link', ''),\n",
        "                            'summary': entry.get('summary', ''),\n",
        "                            'published': entry.get('published', ''),\n",
        "                            'category': category\n",
        "                        }\n",
        "                        self.articles.append(article)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error fetching {feed_url}: {e}\")\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean text for processing\"\"\"\n",
        "        text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def generate_summary(self, text, max_length=100):\n",
        "        \"\"\"Generate concise summary\"\"\"\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        if not sentences:\n",
        "            return \"\"\n",
        "        return sentences[0][:max_length] + (\"...\" if len(sentences[0]) > max_length else \"\")\n",
        "\n",
        "    def personalize_articles(self, user):\n",
        "        \"\"\"Personalize articles for a user based on their interests\"\"\"\n",
        "        user_interests = USER_PERSONAS[user]['interests']\n",
        "        user_categories = USER_PERSONAS[user]['categories']\n",
        "\n",
        "        # Filter articles by category\n",
        "        relevant_articles = [a for a in self.articles if a['category'] in user_categories]\n",
        "        if not relevant_articles:\n",
        "            return []\n",
        "\n",
        "        # Create document corpus for TF-IDF\n",
        "        documents = [self.clean_text(a['title'] + ' ' + a['summary']) for a in relevant_articles]\n",
        "        interest_text = ' '.join(user_interests)\n",
        "\n",
        "        # Compute TF-IDF and similarity\n",
        "        tfidf_matrix = self.vectorizer.fit_transform(documents + [interest_text])\n",
        "        similarities = cosine_similarity(tfidf_matrix[-1:], tfidf_matrix[:-1])[0]\n",
        "\n",
        "        # Rank articles by relevance\n",
        "        ranked_articles = sorted(\n",
        "            zip(relevant_articles, similarities),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        # Select top 5 articles\n",
        "        return [article for article, _ in ranked_articles[:5]]\n",
        "\n",
        "    def generate_newsletter(self, user):\n",
        "        \"\"\"Generate Markdown newsletter for a user\"\"\"\n",
        "        selected_articles = self.personalize_articles(user)\n",
        "        date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "        markdown_content = f\"# Personalized Newsletter for {user}\\n\\n\"\n",
        "        markdown_content += f\"*Generated on {date_str}*\\n\\n\"\n",
        "\n",
        "        if not selected_articles:\n",
        "            markdown_content += \"No relevant articles found for your interests today.\\n\"\n",
        "            return markdown_content\n",
        "\n",
        "        # Add summary section\n",
        "        markdown_content += \"## Today's Highlights\\n\\n\"\n",
        "        for article in selected_articles[:2]:  # Top 2 for highlights\n",
        "            summary = self.generate_summary(article['summary'])\n",
        "            markdown_content += f\"- **{article['title']}**: {summary} [Read more]({article['link']})\\n\"\n",
        "\n",
        "        # Add detailed sections by category\n",
        "        categories = set(a['category'] for a in selected_articles)\n",
        "        for category in categories:\n",
        "            markdown_content += f\"\\n## {category.capitalize()} News\\n\\n\"\n",
        "            for article in [a for a in selected_articles if a['category'] == category]:\n",
        "                summary = self.generate_summary(article['summary'])\n",
        "                markdown_content += f\"### {article['title']}\\n\\n\"\n",
        "                markdown_content += f\"{summary}\\n\\n\"\n",
        "                markdown_content += f\"[Read Full Article]({article['link']})\\n\\n\"\n",
        "\n",
        "        return markdown_content\n",
        "\n",
        "    def save_newsletter(self, user, markdown_content):\n",
        "        \"\"\"Save newsletter as Markdown file\"\"\"\n",
        "        filename = f\"newsletter_{user.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.md\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(markdown_content)\n",
        "        return filename"
      ],
      "metadata": {
        "id": "G8RbTBLIPVRS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def main():\n",
        "    generator = NewsletterGenerator()\n",
        "    print(\"Fetching articles...\")\n",
        "    generator.fetch_articles()\n",
        "\n",
        "    for user in USER_PERSONAS.keys():\n",
        "        print(f\"Generating newsletter for {user}...\")\n",
        "        newsletter = generator.generate_newsletter(user)\n",
        "        filename = generator.save_newsletter(user, newsletter)\n",
        "        print(f\"Newsletter saved as {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE9tHo1DOjKk",
        "outputId": "64763479-3f96-4120-a370-5d5d3b6157a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching articles...\n",
            "Generating newsletter for Alex Parker...\n",
            "Newsletter saved as newsletter_Alex_Parker_20250412.md\n",
            "Generating newsletter for Priya Sharma...\n",
            "Newsletter saved as newsletter_Priya_Sharma_20250412.md\n",
            "Generating newsletter for Marco Rossi...\n",
            "Newsletter saved as newsletter_Marco_Rossi_20250412.md\n",
            "Generating newsletter for Lisa Thompson...\n",
            "Newsletter saved as newsletter_Lisa_Thompson_20250412.md\n",
            "Generating newsletter for David Martinez...\n",
            "Newsletter saved as newsletter_David_Martinez_20250412.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mb-bVP3uPhDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}